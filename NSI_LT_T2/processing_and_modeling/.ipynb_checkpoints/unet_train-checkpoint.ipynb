{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2025-03-08T21:59:44.123564Z",
     "shell.execute_reply": "2025-03-08T21:59:44.122721Z",
     "shell.execute_reply.started": "2025-03-08T21:59:42.926624Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import albumentations as album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T21:59:45.052430Z",
     "iopub.status.busy": "2025-03-08T21:59:45.052051Z",
     "iopub.status.idle": "2025-03-08T21:59:46.505293Z",
     "shell.execute_reply": "2025-03-08T21:59:46.504232Z",
     "shell.execute_reply.started": "2025-03-08T21:59:45.052398Z"
    }
   },
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T21:59:46.507538Z",
     "iopub.status.busy": "2025-03-08T21:59:46.507196Z",
     "iopub.status.idle": "2025-03-08T21:59:46.511756Z",
     "shell.execute_reply": "2025-03-08T21:59:46.510724Z",
     "shell.execute_reply.started": "2025-03-08T21:59:46.507506Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '..data/unet_train/input/inria-aerial-image-labeling-dataset/AerialImageDataset'\n",
    "\n",
    "x_train_dir = os.path.join(DATA_DIR, 'train/images')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train/gt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T21:59:46.513455Z",
     "iopub.status.busy": "2025-03-08T21:59:46.513116Z",
     "iopub.status.idle": "2025-03-08T21:59:46.523810Z",
     "shell.execute_reply": "2025-03-08T21:59:46.522892Z",
     "shell.execute_reply.started": "2025-03-08T21:59:46.513418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dataset classes and their corresponding RGB values in labels:\n",
      "Class Names:  ['background', 'building']\n",
      "Class RGB values:  [[0, 0, 0], [255, 255, 255]]\n"
     ]
    }
   ],
   "source": [
    "class_names = ['background', 'building']\n",
    "class_rgb_values = [[0,0,0],[255,255,255]]\n",
    "\n",
    "print('All dataset classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T21:59:52.647156Z",
     "iopub.status.busy": "2025-03-08T21:59:52.646807Z",
     "iopub.status.idle": "2025-03-08T21:59:52.654454Z",
     "shell.execute_reply": "2025-03-08T21:59:52.653369Z",
     "shell.execute_reply.started": "2025-03-08T21:59:52.647125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected classes and their corresponding RGB values in labels:\n",
      "Class Names:  ['background', 'building']\n",
      "Class RGB values:  [[0, 0, 0], [255, 255, 255]]\n"
     ]
    }
   ],
   "source": [
    "select_class_indices = [class_names.index(cls.lower()) for cls in class_names]\n",
    "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
    "\n",
    "print('Selected classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T21:59:53.048872Z",
     "iopub.status.busy": "2025-03-08T21:59:53.048527Z",
     "iopub.status.idle": "2025-03-08T21:59:53.060348Z",
     "shell.execute_reply": "2025-03-08T21:59:53.059309Z",
     "shell.execute_reply.started": "2025-03-08T21:59:53.048844Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"\n",
    "    Plot images in one row\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([]); \n",
    "        plt.yticks([])\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# perform one hot encoding on label\n",
    "def one_hot_encode(label, label_values):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "\n",
    "    return semantic_map\n",
    "    \n",
    "# perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image):\n",
    "    \"\"\"\n",
    "    Transform a 2D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "# perform colour coding on the reverse-one-hot outputs\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "\n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T21:59:57.847092Z",
     "iopub.status.busy": "2025-03-08T21:59:57.846709Z",
     "iopub.status.idle": "2025-03-08T21:59:57.862485Z",
     "shell.execute_reply": "2025-03-08T21:59:57.861383Z",
     "shell.execute_reply.started": "2025-03-08T21:59:57.847049Z"
    }
   },
   "outputs": [],
   "source": [
    "class BuildingsDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"Inria Arial Image Buildings Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            class_rgb_values=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "            crop_size=512,\n",
    "    ):\n",
    "        \n",
    "        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n",
    "        self.mask_paths = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]\n",
    "\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.crop_size = crop_size\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read images and masks\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        # one-hot-encode the mask\n",
    "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
    "\n",
    "        assert image.shape[:2] == mask.shape[:2], \"Image and mask size mismatch!\"\n",
    "        image, mask = self.random_crop(image, mask, self.crop_size)\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def random_crop(self, image, mask, crop_size):\n",
    "        \"\"\"Randomly crops the same region from both image and mask\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        if h < crop_size or w < crop_size:\n",
    "            raise ValueError(\"Crop size is larger than image size!\")\n",
    "\n",
    "        # Randomly select top-left corner for the crop\n",
    "        x = random.randint(0, w - crop_size)\n",
    "        y = random.randint(0, h - crop_size)\n",
    "\n",
    "        # Crop both image and mask\n",
    "        image_cropped = image[y:y+crop_size, x:x+crop_size]\n",
    "        mask_cropped = mask[y:y+crop_size, x:x+crop_size]\n",
    "\n",
    "        return image_cropped, mask_cropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T22:00:00.856743Z",
     "iopub.status.busy": "2025-03-08T22:00:00.856365Z",
     "iopub.status.idle": "2025-03-08T22:00:00.869611Z",
     "shell.execute_reply": "2025-03-08T22:00:00.868457Z",
     "shell.execute_reply.started": "2025-03-08T22:00:00.856709Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [    \n",
    "        album.OneOf(\n",
    "            [\n",
    "                album.HorizontalFlip(p=1),\n",
    "                album.VerticalFlip(p=1),\n",
    "                album.RandomRotate90(p=1),\n",
    "            ],\n",
    "            p=0.75,\n",
    "        ),\n",
    "    ]\n",
    "    return album.Compose(train_transform, additional_targets={'mask': 'mask'})\n",
    "\n",
    "\n",
    "def get_validation_augmentation():   \n",
    "    # Add sufficient padding to ensure image is divisible by 32\n",
    "    test_transform = [\n",
    "        album.PadIfNeeded(min_height=512, min_width=512, always_apply=True, border_mode=0),\n",
    "    ]\n",
    "    return album.Compose(test_transform, additional_targets={'mask': 'mask'})\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def grayscale_preprocessing(image, **kwargs):\n",
    "    \"\"\"\n",
    "    Convert an image to grayscale.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy array): Input image in RGB format.\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Grayscale image with shape (H, W, 1).\n",
    "    \"\"\"\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    image_gray = np.expand_dims(image_gray, axis=-1)\n",
    "    image_gray = np.repeat(image_gray, 3, axis=-1)\n",
    "    return image_gray\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):\n",
    "    \"\"\"Construct preprocessing transform    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \"\"\"   \n",
    "    _transform = []\n",
    "    if preprocessing_fn:\n",
    "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
    "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
    "\n",
    "    return album.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T22:00:03.353621Z",
     "iopub.status.busy": "2025-03-08T22:00:03.353242Z",
     "iopub.status.idle": "2025-03-08T22:00:03.595454Z",
     "shell.execute_reply": "2025-03-08T22:00:03.594379Z",
     "shell.execute_reply.started": "2025-03-08T22:00:03.353589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class InceptionResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionResNetBlock, self).__init__()\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 4, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 4, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels // 4, out_channels // 4, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 4, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels // 4, out_channels // 4, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(out_channels // 4 * 3, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # ensure residual connection matches output dimensions\n",
    "        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.residual_bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual_bn(self.residual_conv(x))  # Ensure matching dimensions\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.bn(x)\n",
    "        x += residual\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class UNetDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetDecoderBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = ConvBlock(out_channels * 2, out_channels)\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class InceptionResNetUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=2):\n",
    "        super(InceptionResNetUNet, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.enc1 = InceptionResNetBlock(in_channels, 64)\n",
    "        self.enc2 = InceptionResNetBlock(64, 128)\n",
    "        self.enc3 = InceptionResNetBlock(128, 256)\n",
    "        self.enc4 = InceptionResNetBlock(256, 512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # bottleneck\n",
    "        self.bottleneck = InceptionResNetBlock(512, 1024)\n",
    "        \n",
    "        # decoder\n",
    "        self.dec4 = UNetDecoderBlock(1024, 512)\n",
    "        self.dec3 = UNetDecoderBlock(512, 256)\n",
    "        self.dec2 = UNetDecoderBlock(256, 128)\n",
    "        self.dec1 = UNetDecoderBlock(128, 64)\n",
    "        \n",
    "        # output Layer\n",
    "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool(enc1))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "        \n",
    "        bottleneck = self.bottleneck(self.pool(enc4))\n",
    "        \n",
    "        dec4 = self.dec4(bottleneck, enc4)\n",
    "        dec3 = self.dec3(dec4, enc3)\n",
    "        dec2 = self.dec2(dec3, enc2)\n",
    "        dec1 = self.dec1(dec2, enc1)\n",
    "        \n",
    "        out = self.out_conv(dec1)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "# initalize\n",
    "model = InceptionResNetUNet(in_channels=3, out_channels=2)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.eval()\n",
    "    print('loaded model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T22:00:10.347080Z",
     "iopub.status.busy": "2025-03-08T22:00:10.346726Z",
     "iopub.status.idle": "2025-03-08T22:00:10.365977Z",
     "shell.execute_reply": "2025-03-08T22:00:10.364935Z",
     "shell.execute_reply.started": "2025-03-08T22:00:10.347049Z"
    }
   },
   "outputs": [],
   "source": [
    "# define proportions\n",
    "train_ratio = 0.9\n",
    "val_ratio = 0.05\n",
    "test_ratio = 0.05\n",
    "\n",
    "full_dataset = BuildingsDataset(\n",
    "    x_train_dir, y_train_dir, \n",
    "    class_rgb_values=select_class_rgb_values,\n",
    "    augmentation=None,\n",
    "    preprocessing=None\n",
    ")\n",
    "\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size  \n",
    "\n",
    "train_indices, val_indices, test_indices = random_split(range(total_size), [train_size, val_size, test_size])\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "valid_dataset = Subset(full_dataset, val_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "train_dataset.dataset.augmentation = get_training_augmentation()\n",
    "valid_dataset.dataset.augmentation = get_validation_augmentation()\n",
    "test_dataset.dataset.augmentation = get_validation_augmentation()\n",
    "\n",
    "train_dataset.dataset.preprocessing = get_preprocessing(preprocessing_fn=None)\n",
    "valid_dataset.dataset.preprocessing = get_preprocessing(preprocessing_fn=None)\n",
    "test_dataset.dataset.preprocessing = get_preprocessing(preprocessing_fn=None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=12, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T22:00:27.657329Z",
     "iopub.status.busy": "2025-03-08T22:00:27.656900Z",
     "iopub.status.idle": "2025-03-08T22:00:27.665133Z",
     "shell.execute_reply": "2025-03-08T22:00:27.664068Z",
     "shell.execute_reply.started": "2025-03-08T22:00:27.657285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
    "TRAINING = True\n",
    "\n",
    "# Set num of epochs\n",
    "EPOCHS = 40\n",
    "\n",
    "# Set device: `cuda` or `cpu`\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "# define loss function\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "# define metrics\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.7),\n",
    "]\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.00008),\n",
    "])\n",
    "\n",
    "# define learning rate scheduler (not used in this NB)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T22:00:37.046845Z",
     "iopub.status.busy": "2025-03-08T22:00:37.046263Z",
     "iopub.status.idle": "2025-03-08T22:00:40.187706Z",
     "shell.execute_reply": "2025-03-08T22:00:40.186853Z",
     "shell.execute_reply.started": "2025-03-08T22:00:37.046801Z"
    }
   },
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T22:00:40.190449Z",
     "iopub.status.busy": "2025-03-08T22:00:40.189968Z",
     "iopub.status.idle": "2025-03-08T22:00:40.195478Z",
     "shell.execute_reply": "2025-03-08T22:00:40.194369Z",
     "shell.execute_reply.started": "2025-03-08T22:00:40.190400Z"
    }
   },
   "outputs": [],
   "source": [
    "best_iou_score = 0.0\n",
    "train_logs_list, valid_logs_list = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2025-03-08T22:21:50.626895Z",
     "iopub.status.busy": "2025-03-08T22:21:50.626512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train: 11it [02:24, 13.12s/it, dice_loss - 0.3426, iou_score - 0.475] \n",
      "valid: 9it [00:07,  1.13it/s, dice_loss - 0.3062, iou_score - 0.606] \n",
      "Model saved!\n",
      "\n",
      "Epoch: 1\n",
      "train: 11it [02:22, 12.92s/it, dice_loss - 0.3304, iou_score - 0.5148]\n",
      "valid: 9it [00:08,  1.03it/s, dice_loss - 0.3177, iou_score - 0.5887]\n",
      "\n",
      "Epoch: 2\n",
      "train: 11it [02:33, 13.93s/it, dice_loss - 0.3191, iou_score - 0.5513]\n",
      "valid: 9it [00:08,  1.05it/s, dice_loss - 0.3347, iou_score - 0.5132]\n",
      "\n",
      "Epoch: 3\n",
      "train: 11it [02:22, 12.98s/it, dice_loss - 0.3152, iou_score - 0.5563]\n",
      "valid: 9it [00:07,  1.13it/s, dice_loss - 0.3153, iou_score - 0.5333]\n",
      "\n",
      "Epoch: 4\n",
      "train: 11it [02:22, 12.96s/it, dice_loss - 0.3091, iou_score - 0.6368]\n",
      "valid: 9it [00:07,  1.13it/s, dice_loss - 0.3305, iou_score - 0.5878]\n",
      "\n",
      "Epoch: 5\n",
      "train: 11it [02:22, 12.96s/it, dice_loss - 0.3031, iou_score - 0.6711]\n",
      "valid: 9it [00:07,  1.13it/s, dice_loss - 0.336, iou_score - 0.5994] \n",
      "\n",
      "Epoch: 6\n",
      "train: 11it [02:23, 13.07s/it, dice_loss - 0.2915, iou_score - 0.7006]\n",
      "valid: 9it [00:08,  1.09it/s, dice_loss - 0.2946, iou_score - 0.6701]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 7\n",
      "train: 11it [02:20, 12.82s/it, dice_loss - 0.2981, iou_score - 0.6839]\n",
      "valid: 9it [00:07,  1.14it/s, dice_loss - 0.3014, iou_score - 0.6868]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 8\n",
      "train: 11it [02:22, 12.95s/it, dice_loss - 0.292, iou_score - 0.6923] \n",
      "valid: 9it [00:07,  1.15it/s, dice_loss - 0.2786, iou_score - 0.7573]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 9\n",
      "train: 11it [02:21, 12.89s/it, dice_loss - 0.2744, iou_score - 0.7532]\n",
      "valid: 9it [00:08,  1.08it/s, dice_loss - 0.2805, iou_score - 0.7664]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 10\n",
      "train: 11it [02:20, 12.76s/it, dice_loss - 0.2756, iou_score - 0.7381]\n",
      "valid: 9it [00:08,  1.12it/s, dice_loss - 0.2519, iou_score - 0.7661]\n",
      "\n",
      "Epoch: 11\n",
      "train: 11it [02:21, 12.84s/it, dice_loss - 0.2718, iou_score - 0.7456]\n",
      "valid: 9it [00:07,  1.13it/s, dice_loss - 0.2593, iou_score - 0.792] \n",
      "Model saved!\n",
      "\n",
      "Epoch: 12\n",
      "train: 11it [02:21, 12.89s/it, dice_loss - 0.2722, iou_score - 0.7512]\n",
      "valid: 9it [00:08,  1.07it/s, dice_loss - 0.2888, iou_score - 0.7119]\n",
      "\n",
      "Epoch: 13\n",
      "train: 11it [02:21, 12.83s/it, dice_loss - 0.2656, iou_score - 0.7572]\n",
      "valid: 9it [00:08,  1.12it/s, dice_loss - 0.2406, iou_score - 0.7832]\n",
      "\n",
      "Epoch: 14\n",
      "train: 11it [02:22, 12.97s/it, dice_loss - 0.2636, iou_score - 0.7584]\n",
      "valid: 9it [00:07,  1.14it/s, dice_loss - 0.2818, iou_score - 0.7189]\n",
      "\n",
      "Epoch: 15\n",
      "train: 11it [02:22, 12.99s/it, dice_loss - 0.2617, iou_score - 0.7591]\n",
      "valid: 9it [00:07,  1.13it/s, dice_loss - 0.2798, iou_score - 0.674] \n",
      "\n",
      "Epoch: 16\n",
      "train: 11it [02:22, 12.99s/it, dice_loss - 0.2492, iou_score - 0.7932]\n",
      "valid: 9it [00:08,  1.11it/s, dice_loss - 0.2467, iou_score - 0.805] \n",
      "Model saved!\n",
      "\n",
      "Epoch: 17\n",
      "train: 11it [02:19, 12.71s/it, dice_loss - 0.2453, iou_score - 0.7977]\n",
      "valid: 9it [00:07,  1.14it/s, dice_loss - 0.2862, iou_score - 0.6934]\n",
      "\n",
      "Epoch: 18\n",
      "train: 11it [02:21, 12.86s/it, dice_loss - 0.2516, iou_score - 0.7726]\n",
      "valid: 9it [00:07,  1.14it/s, dice_loss - 0.226, iou_score - 0.8362] \n",
      "Model saved!\n",
      "\n",
      "Epoch: 19\n",
      "train: 11it [02:23, 13.02s/it, dice_loss - 0.2458, iou_score - 0.7875]\n",
      "valid: 9it [00:08,  1.08it/s, dice_loss - 0.2857, iou_score - 0.6867]\n",
      "\n",
      "Epoch: 20\n",
      "train: 11it [02:21, 12.85s/it, dice_loss - 0.2455, iou_score - 0.7867]\n",
      "valid: 9it [00:07,  1.13it/s, dice_loss - 0.2579, iou_score - 0.7391]\n",
      "\n",
      "Epoch: 21\n",
      "train: 11it [02:19, 12.71s/it, dice_loss - 0.241, iou_score - 0.7826] \n",
      "valid: 9it [00:07,  1.14it/s, dice_loss - 0.244, iou_score - 0.7773] \n",
      "\n",
      "Epoch: 22\n",
      "train: 11it [02:20, 12.81s/it, dice_loss - 0.2403, iou_score - 0.7826]\n",
      "valid: 9it [00:08,  1.07it/s, dice_loss - 0.267, iou_score - 0.714]  \n",
      "\n",
      "Epoch: 23\n",
      "train: 11it [02:21, 12.85s/it, dice_loss - 0.2354, iou_score - 0.7942]\n",
      "valid: 9it [00:07,  1.13it/s, dice_loss - 0.247, iou_score - 0.769]  \n",
      "\n",
      "Epoch: 24\n",
      "train: 11it [02:19, 12.67s/it, dice_loss - 0.2312, iou_score - 0.8096]\n",
      "valid: 9it [00:07,  1.13it/s, dice_loss - 0.2458, iou_score - 0.7627]\n",
      "\n",
      "Epoch: 25\n",
      "train: 11it [02:21, 12.89s/it, dice_loss - 0.2315, iou_score - 0.7828]\n",
      "valid: 9it [00:08,  1.09it/s, dice_loss - 0.2364, iou_score - 0.7957]\n",
      "\n",
      "Epoch: 26\n",
      "train: 11it [02:20, 12.76s/it, dice_loss - 0.2323, iou_score - 0.7892]\n",
      "valid: 9it [00:07,  1.14it/s, dice_loss - 0.2277, iou_score - 0.8066]\n",
      "\n",
      "Epoch: 27\n",
      "train: 11it [02:20, 12.81s/it, dice_loss - 0.2243, iou_score - 0.7978]\n",
      "valid: 9it [00:07,  1.14it/s, dice_loss - 0.215, iou_score - 0.7929] \n",
      "\n",
      "Epoch: 28\n",
      "train: 7it [02:12, 15.98s/it, dice_loss - 0.2176, iou_score - 0.8066] "
     ]
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    train_device = torch.device(\"cuda:0\")\n",
    "    valid_device = torch.device(\"cuda:1\")\n",
    "\n",
    "    model.to(train_device)\n",
    "\n",
    "    for i in range(0, EPOCHS):\n",
    "\n",
    "        print('\\nEpoch: {}'.format(i))\n",
    "        train_logs = train_epoch.run((inputs.to(train_device), targets.to(train_device)) for inputs, targets in train_loader)\n",
    "        train_logs_list.append(train_logs)\n",
    "        del train_logs\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "            valid_logs = valid_epoch.run((inputs.to(valid_device), targets.to(valid_device)) for inputs, targets in valid_loader)\n",
    "        \n",
    "            valid_logs_list.append(valid_logs)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if best_iou_score < valid_logs['iou_score']:\n",
    "            best_iou_score = valid_logs['iou_score']\n",
    "            torch.save(model, './best_model.pth')\n",
    "            print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved model checkpoint from the current run\n",
    "if os.path.exists('./best_model.pth'):\n",
    "    best_model = torch.load('./best_model.pth', map_location=DEVICE)\n",
    "    print('Loaded UNet model from this run.')\n",
    "\n",
    "# load best saved model checkpoint from previous commit (if present)\n",
    "elif os.path.exists('../input/unet-for-building-segmentation-pytorch/best_model.pth'):\n",
    "    best_model = torch.load('../input/unet-for-building-segmentation-pytorch/best_model.pth', map_location=DEVICE)\n",
    "    print('Loaded UNet model from a previous commit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center crop padded image / mask to original image dims\n",
    "def crop_image(image, target_image_dims=[512,512,3]):\n",
    "   \n",
    "    target_size = target_image_dims[0]\n",
    "    image_size = len(image)\n",
    "    padding = (image_size - target_size) // 2\n",
    "\n",
    "    return image[\n",
    "        padding:image_size - padding,\n",
    "        padding:image_size - padding,\n",
    "        :,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_preds_folder = '../data/unet_train/sample_predictions/'\n",
    "if not os.path.exists(sample_preds_folder):\n",
    "    os.makedirs(sample_preds_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(test_dataset)):\n",
    "    \n",
    "    random_idx = random.randint(0, len(test_dataset)-1)\n",
    "    image, gt_mask = test_dataset[random_idx]\n",
    "    image_vis = crop_image(image.astype('uint8'))\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "\n",
    "    pred_mask = best_model(x_tensor)\n",
    "    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
    "    # Convert pred_mask from `CHW` format to `HWC` format\n",
    "    pred_mask = np.transpose(pred_mask,(1,2,0))\n",
    "    # Get prediction channel corresponding to building\n",
    "    pred_building_heatmap = pred_mask[:,:,0]\n",
    "    pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values))\n",
    "    # Convert gt_mask from `CHW` format to `HWC` format\n",
    "    gt_mask = np.transpose(gt_mask,(1,2,0))\n",
    "    gt_mask = crop_image(colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values))\n",
    "    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.png\"), np.hstack([image_vis, gt_mask, pred_mask, pred_building_heatmap])[:,:,::-1])\n",
    "    \n",
    "    visualize(\n",
    "        original_image = image_vis,\n",
    "        ground_truth_mask = gt_mask,\n",
    "        predicted_mask = pred_mask,\n",
    "        predicted_building_heatmap = pred_building_heatmap\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 892049,
     "sourceId": 1514093,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6818489,
     "sourceId": 10959980,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 51899294,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30014,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
