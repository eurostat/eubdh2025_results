{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8cc4c8",
   "metadata": {},
   "source": [
    "# Segment and Embed geotif orthophotographs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f43ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.features import shapes\n",
    "from rasterio.mask import mask as rasterio_mask\n",
    "from rasterio.plot import show\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "import albumentations as album\n",
    "import segmentation_models_pytorch as smp\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "import json\n",
    "import osmnx as ox\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19045892-f245-4c68-81a3-ef90df8e3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a384cd9-1152-4424-aba1-e9881bba02dd",
   "metadata": {},
   "source": [
    "# Slice geotiff for UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda730fd-529e-4cbf-821e-28b70854e36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TILE_SIZE = 512\n",
    "\n",
    "def slice_geotiff(input_tiff, output_folder, tile_size=TILE_SIZE):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    with rasterio.open(input_tiff) as src:\n",
    "        meta = src.meta.copy()\n",
    "\n",
    "        img_width, img_height = src.width, src.height\n",
    "\n",
    "        x_tiles = img_width // tile_size\n",
    "        y_tiles = img_height // tile_size\n",
    "\n",
    "        if img_width % tile_size != 0:\n",
    "            x_tiles += 1\n",
    "        if img_height % tile_size != 0:\n",
    "            y_tiles += 1\n",
    "\n",
    "        print(f\"Slicing into {x_tiles}x{y_tiles} tiles\")\n",
    "\n",
    "        for i in range(x_tiles):\n",
    "            for j in range(y_tiles):\n",
    "                # compute window boundaries\n",
    "                x_offset = i * tile_size\n",
    "                y_offset = j * tile_size\n",
    "                width = min(tile_size, img_width - x_offset)\n",
    "                height = min(tile_size, img_height - y_offset)\n",
    "\n",
    "                window = Window(x_offset, y_offset, width, height)\n",
    "                data = src.read(window=window)\n",
    "\n",
    "                meta.update({\n",
    "                    'width': width,\n",
    "                    'height': height,\n",
    "                    'transform': rasterio.windows.transform(window, src.transform)\n",
    "                })\n",
    "\n",
    "                inp_tif_title = input_tiff.split('/')[-1].split('.')[0]\n",
    "                output_path = os.path.join(output_folder, f\"{inp_tif_title}_tile_{i}_{j}.tif\")\n",
    "                with rasterio.open(output_path, \"w\", **meta) as dst:\n",
    "                    dst.write(data)\n",
    "\n",
    "                print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f79584-7cfe-4d20-812d-89ead994fcc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    {'input_folder': '../data/EE/orto/2020', 'output_folder': '../data/sliced/EE/2020', 'vector_outputs': '../data/sliced/EE/2020_vectors'},\n",
    "    {'input_folder': '../data/EE/orto/2024', 'output_folder': '../data/sliced/EE/2024', 'vector_outputs': '../data/sliced/EE/2024_vectors'},\n",
    "    {'input_folder': '../data/LT/orto/2020', 'output_folder': '../data/sliced/LT/2020', 'vector_outputs': '../data/sliced/LT/2020_vectors'},\n",
    "    {'input_folder': '../data/LT/orto/2024', 'output_folder': '../data/sliced/LT/2024', 'vector_outputs': '../data/sliced/LT/2024_vectors'},\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    input_folder = dataset['input_folder']\n",
    "    output_folder = dataset['output_folder']\n",
    "    tif_files = glob.glob(os.path.join(input_folder, \"*.tif\"))\n",
    "    for input_tiff in tif_files:\n",
    "        slice_geotiff(input_tiff, output_folder)\n",
    "\n",
    "print('finished slicing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab01ce2f",
   "metadata": {},
   "source": [
    "# Segmentation using UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df35e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class InceptionResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionResNetBlock, self).__init__()\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 4, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 4, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels // 4, out_channels // 4, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 4, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels // 4, out_channels // 4, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(out_channels // 4 * 3, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.residual_bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual_bn(self.residual_conv(x))\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.bn(x)\n",
    "        x += residual\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class UNetDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetDecoderBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = ConvBlock(out_channels * 2, out_channels)\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class InceptionResNetUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=2):\n",
    "        super(InceptionResNetUNet, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.enc1 = InceptionResNetBlock(in_channels, 64)\n",
    "        self.enc2 = InceptionResNetBlock(64, 128)\n",
    "        self.enc3 = InceptionResNetBlock(128, 256)\n",
    "        self.enc4 = InceptionResNetBlock(256, 512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # bottleneck\n",
    "        self.bottleneck = InceptionResNetBlock(512, 1024)\n",
    "        \n",
    "        # decoder\n",
    "        self.dec4 = UNetDecoderBlock(1024, 512)\n",
    "        self.dec3 = UNetDecoderBlock(512, 256)\n",
    "        self.dec2 = UNetDecoderBlock(256, 128)\n",
    "        self.dec1 = UNetDecoderBlock(128, 64)\n",
    "        \n",
    "        # output Layer\n",
    "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool(enc1))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "        \n",
    "        bottleneck = self.bottleneck(self.pool(enc4))\n",
    "        \n",
    "        dec4 = self.dec4(bottleneck, enc4)\n",
    "        dec3 = self.dec3(dec4, enc3)\n",
    "        dec2 = self.dec2(dec3, enc2)\n",
    "        dec1 = self.dec1(dec2, enc1)\n",
    "        \n",
    "        out = self.out_conv(dec1)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "model = InceptionResNetUNet(in_channels=3, out_channels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67976430",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "if os.path.exists('./best_model.pth'):\n",
    "    model = torch.load('./best_model.pth', map_location=DEVICE, weights_only=False)\n",
    "    # model trained on parallel GPU - if not, can skip\n",
    "    model = model.module\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    print('Loaded UNet model from file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64560123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_unseen_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    transform = album.Compose([\n",
    "        album.PadIfNeeded(min_height=512, min_width=512, always_apply=True, border_mode=0),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    transformed = transform(image=image)\n",
    "    image_tensor = transformed[\"image\"].unsqueeze(0)\n",
    "    \n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, target_image_dims=[512,512,3]):\n",
    "   \n",
    "    target_size = target_image_dims[0]\n",
    "    image_size = len(image)\n",
    "    padding = (image_size - target_size) // 2\n",
    "\n",
    "    return image[\n",
    "        padding:image_size - padding,\n",
    "        padding:image_size - padding,\n",
    "        :,\n",
    "    ]\n",
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"\n",
    "    Plot images in one row\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([]); \n",
    "        plt.yticks([])\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Perform one hot encoding on label\n",
    "def one_hot_encode(label, label_values):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "\n",
    "    return semantic_map\n",
    "    \n",
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image):\n",
    "    \"\"\"\n",
    "    Transform a 2D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "# Perform colour coding on the reverse-one-hot outputs\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "\n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[(image > 0.35).astype(int)]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ddb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['background', 'building']\n",
    "class_rgb_values = [[0,0,0],[255,255,255]]\n",
    "\n",
    "select_class_indices = [class_names.index(cls.lower()) for cls in class_names]\n",
    "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd225fe-e966-491e-8f32-5ab09f5bf182",
   "metadata": {},
   "source": [
    "# Process orthophotograph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cdea9f-f4c8-45e3-b549-e65f503742d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    input_folder = dataset['output_folder']\n",
    "    vector_outputs = dataset['vector_outputs']\n",
    "    \n",
    "    tif_files = glob.glob(os.path.join(input_folder, \"*.tif\"))\n",
    "    \n",
    "    gdfs = []\n",
    "    \n",
    "    for i, tif_file in enumerate(tif_files):\n",
    "        print(f'Processing: {tif_file}', end='')\n",
    "    \n",
    "        image_path = tif_file\n",
    "        img = preprocess_unseen_image(image_path)\n",
    "        img = img.to(DEVICE)\n",
    "        \n",
    "        img = img.to(torch.float32)\n",
    "        \n",
    "        pred_mask = model(img)\n",
    "        \n",
    "        pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
    "        # Convert pred_mask from `CHW` format to `HWC` format\n",
    "        pred_mask = np.transpose(pred_mask,(1,2,0))\n",
    "        \n",
    "        pred_building_heatmap = pred_mask[:,:,0]\n",
    "        pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values))\n",
    "        \n",
    "        # ---------------------------------------------\n",
    "        # crop mask to original size\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask_h, mask_w, _ = pred_mask.shape\n",
    "        img_h, img_w, _ = image.shape\n",
    "        \n",
    "        crop_w_start = (mask_w - img_w) // 2 \n",
    "        crop_w_end = crop_w_start + img_w \n",
    "        \n",
    "        crop_h_start = (mask_h - img_h) // 2 \n",
    "        crop_h_end = crop_h_start + img_h \n",
    "        \n",
    "        cropped_mask = pred_mask[crop_h_start:crop_h_end, crop_w_start:crop_w_end]\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        # ---------------------------------------------\n",
    "        # edge cleaning/separation\n",
    "        \n",
    "        small_kernel = np.ones((2,2), np.uint8)\n",
    "        tiny_kernel = np.ones((1,1), np.uint8)\n",
    "        \n",
    "        cropped_mask = cropped_mask.astype(np.uint8)\n",
    "        cropped_mask_gray = cv2.cvtColor(cropped_mask, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        mask = cv2.erode(cropped_mask_gray, small_kernel, iterations=2) \n",
    "        mask = cv2.dilate(mask, small_kernel, iterations=1) \n",
    "        \n",
    "        opened_mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, tiny_kernel, iterations=3)\n",
    "        \n",
    "        _, binary_mask = cv2.threshold(opened_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        dist_transform = cv2.distanceTransform(binary_mask, cv2.DIST_L2, 3)\n",
    "        _, sure_fg = cv2.threshold(dist_transform, 0.40 * dist_transform.max(), 255, 0)\n",
    "        \n",
    "        sure_bg = cv2.dilate(binary_mask, small_kernel, iterations=2)\n",
    "        sure_fg = np.uint8(sure_fg)\n",
    "        unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "        \n",
    "        _, markers = cv2.connectedComponents(sure_fg)\n",
    "        markers = markers + 1\n",
    "        markers[unknown == 255] = 0\n",
    "        cv2.watershed(cropped_mask, markers)\n",
    "        \n",
    "        separated_mask = np.uint8(markers > 1) * 255\n",
    "        \n",
    "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(separated_mask, connectivity=8)\n",
    "        \n",
    "        min_size = 500\n",
    "        filtered_mask = np.zeros_like(separated_mask)\n",
    "        \n",
    "        # skip background\n",
    "        for i in range(1, num_labels):\n",
    "            area = stats[i, cv2.CC_STAT_AREA]\n",
    "            if area >= min_size:\n",
    "                filtered_mask[labels == i] = 255\n",
    "        \n",
    "        filtered_mask_colored = cv2.cvtColor(filtered_mask, cv2.COLOR_GRAY2BGR)\n",
    "        mask_color = np.array([255, 0, 0], dtype=np.uint8)\n",
    "        overlay = np.where(filtered_mask_colored == 255, mask_color, image)\n",
    "        alpha = 0.5\n",
    "        blended = cv2.addWeighted(image, 1 - alpha, overlay, alpha, 0)\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        # ---------------------------------------------\n",
    "        # save as vector data\n",
    "        def raster_to_vector(filtered_mask:\n",
    "            \"\"\"\n",
    "            Convert a raster (.tif) mask to vector polygons and save as GeoJSON.\n",
    "        \n",
    "            Parameters:\n",
    "                input_tif (str): Path to the input .tif file.\n",
    "            \"\"\"\n",
    "            with rasterio.open(image_path) as src:\n",
    "                image = src.read(1)\n",
    "                transform = src.transform\n",
    "                raster_crs = src.crs\n",
    "                raster_shape = (src.height, src.width)\n",
    "                raster_bounds = src.bounds\n",
    "        \n",
    "            if filtered_mask.shape != raster_shape:\n",
    "                raise ValueError('Mask dimensions do not match the original image dimensions')\n",
    "        \n",
    "            shapes_gen = shapes(filtered_mask, transform=transform)\n",
    "        \n",
    "            polygons = [shape(geom) for geom, value in shapes_gen if value > 0]\n",
    "            gdf = gpd.GeoDataFrame(geometry=polygons, crs=raster_crs)\n",
    "    \n",
    "            def plot_raster_with_vectors(raster_path, vector_gdf):\n",
    "                \"\"\"\n",
    "                Plots a raster (.tif) with an overlaid vector layer.\n",
    "            \n",
    "                Parameters:\n",
    "                    raster_path (str): Path to the raster file (.tif).\n",
    "                    vector_gdf (GeoDataFrame): Geopandas GeoDataFrame containing vector polygons.\n",
    "                \"\"\"\n",
    "                with rasterio.open(raster_path) as src:\n",
    "                    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "                    show(src, ax=ax, title='Raster with Vector Overlay', cmap='gray')\n",
    "                    vector_gdf.boundary.plot(ax=ax, edgecolor='red', linewidth=1)\n",
    "                    plt.show()\n",
    "    \n",
    "            print('Vector data generated')\n",
    "            return gdf\n",
    "        \n",
    "        gdf = raster_to_vector(filtered_mask)\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        # get visual data, get resnet50 embeddings\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        resnet50 = torch.nn.Sequential(*list(resnet50.children())[:-1])\n",
    "        resnet50.eval()\n",
    "        \n",
    "        def preprocess_image(image):\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "            return transform(image).unsqueeze(0)\n",
    "        \n",
    "        def extract_features(image):\n",
    "            image_tensor = preprocess_image(image)\n",
    "            with torch.no_grad():\n",
    "                features = resnet50(image_tensor)\n",
    "            return features.squeeze().numpy()\n",
    "        \n",
    "        def embed_building(poly):\n",
    "            cropped_image, cropped_transform = rasterio_mask(src, [poly], crop=True)\n",
    "            cropped_image = np.transpose(cropped_image, (1, 2, 0))\n",
    "            cropped_pil = Image.fromarray(cropped_image.astype(np.uint8))\n",
    "            features = extract_features(cropped_pil)\n",
    "            return features\n",
    "        \n",
    "        \n",
    "        with rasterio.open(image_path) as src:\n",
    "            crs = src.crs\n",
    "            if gdf.crs != crs:\n",
    "                gdf = gdf.to_crs(crs)\n",
    "        \n",
    "            gdf['resnet50_embed'] = gdf['geometry'].apply(lambda poly: embed_building(poly))     \n",
    "    \n",
    "        gdfs.append(gdf)\n",
    "    \n",
    "        if len(gdfs) > 100:\n",
    "            file_name = tif_file.split('/')[-1].split('.')[0]\n",
    "            (\n",
    "                gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n",
    "                # convert coordinate system to WGS84\n",
    "                .to_crs(epsg=4326)\n",
    "                .to_file(f\"{vecotr_outputs}/{file_name}.geojson\", driver='GeoJSON')\n",
    "            )\n",
    "            gdfs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b65248-63fd-4cea-8c33-b271584ec041",
   "metadata": {},
   "source": [
    "# -- Image processed --"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
