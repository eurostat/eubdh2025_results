{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef0d190f-b48b-4d1a-b976-f8c7b289719a",
   "metadata": {},
   "source": [
    "# Reduce features via autoencoder, apply Isolation Forest as a anomaly detection model, calculate extra stats for the resulting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30578dc-8a70-448d-b2c8-e3a3cadfa1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241775b4-ecec-4c2f-9cd3-445d9eac3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shelter_data = gpd.read_file('../data/lt_ee_data_for_model.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ab08e-f8cb-4ca8-bfb5-ce6e27aa8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_gdf = shelter_data[['resnet50_emb']]\n",
    "expanded_df = emb_gdf['resnet50_emb'].apply(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b8d24-6a01-4d5b-a549-c2a8a0b85973",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = expanded_df.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9344568-2387-4456-9988-70d405ddf42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = expanded_df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa377ac-a0a4-4a28-a245-619e8b0ca0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.columns = [f\"resnet50_emb_{i}\" for i in range(expanded_df.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b47ca-8201-4517-a046-de9e32fe7405",
   "metadata": {},
   "source": [
    "### Structured data can be expanded with other tabular parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b99ed3-cac6-4e1e-a692-d2a2ae632466",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_df = shelter_data[['Shape_Length', 'Shape_Area', 'type', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25150bb-7153-4d52-99f9-f20009616442",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = ['Shape_Length', 'Shape_Area']\n",
    "categorical_columns = ['type']\n",
    "embedding_columns = expanded_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5116cc0-292f-4c4f-a1bb-40d40421135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_cats = encoder.fit_transform(structured_df[categorical_columns])\n",
    "encoded_cat_df = gpd.GeoDataFrame(encoded_cats, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_params = scaler.fit_transform(structured_df[numeric_columns])\n",
    "numeric_df = gpd.GeoDataFrame(scaled_params, columns=numeric_columns)\n",
    "\n",
    "combined_df = gpd.GeoDataFrame(pd.concat([numeric_df, encoded_cat_df, expanded_df], axis=1), geometry=structured_df.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eecccc7-4de4-4b0f-b63c-71d2334a0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_df.drop('geometry', axis=1).to_numpy()\n",
    "y = shelter_data['is_shelter'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59cabf-83e3-41cd-8025-e38bd31c53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_targets = X[y == 1]\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "X_targets_tensor = torch.tensor(X_targets, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e6e7a-adda-4996-a2b0-0c8405947a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def train_autoencoder(model, data, epochs=60, batch_size=32, lr=0.0008):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(data, data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            inputs, _ = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.6f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e62b38-0fb9-44fc-a1d9-f52f236fc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_targets.shape[1]\n",
    "autoencoder = Autoencoder(input_dim)\n",
    "train_autoencoder(autoencoder, X_targets_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2f436-2dac-4fd2-98d8-009e82184e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    X_reconstructed_tensor = autoencoder(X_tensor)\n",
    "    X_encoded_tensor = autoencoder.encoder(X_tensor)\n",
    "reconstruction_errors = torch.mean((X_tensor - X_reconstructed_tensor) ** 2, dim=1).numpy()\n",
    "X_encoded = X_encoded_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326516e-b88e-4a5e-8e33-7ff512b7a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iforest_train = X_encoded[y == 1]\n",
    "iso_forest = IsolationForest(n_estimators=500, contamination=0.01, random_state=42)\n",
    "iso_forest.fit(X_iforest_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4521314-21e5-4255-8cc6-66f928e197ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_scores = iso_forest.decision_function(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4450f7c-5c0e-49c2-8480-5cc1e51eb965",
   "metadata": {},
   "outputs": [],
   "source": [
    "shelter_similarity_scores = iso_scores - reconstruction_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c45436-d227-4562-98af-5d77c66617e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_like_df = shelter_data[['geometry', 'is_shelter']]\n",
    "target_like_df['shelter_like_score'] = shelter_similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09f7cf-9d7c-4e79-b419-1bcbf1ad9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust for sensitivity - hiher percentile -> more sensitive\n",
    "q2_shelters = np.percentile(shelter_similarity_scores[y == 1], 75)\n",
    "threshold = q2_shelters\n",
    "target_like_df['could_be_shelter'] = (target_like_df['shelter_like_score'] >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e279ed3-c0e7-40c7-9873-aeb69d38f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shelter_data[['shelter_like_score', 'is_shelter', 'could_be_shelter']] = target_like_df[['shelter_like_score', 'is_shelter', 'could_be_shelter']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e0638-4091-42de-9fa7-98bed6b0b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "shelter_data.drop_duplicates(subset=['geometry', 'year']).to_file('../data/possible_shelters.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c769fee-d3d4-4002-a841-1a2c00d06056",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts = gpd.read_file('../data/NUTS_RG_20M_2024_4326.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81dffe0-ed29-4028-854e-aff832e73d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts = nuts[nuts['NUTS_NAME'].isin(['Vilniaus apskritis', 'PÃµhja-Eesti'])][['geometry', 'CNTR_CODE', 'NUTS_ID', 'NUTS_NAME']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c23bb4-e377-4431-89c7-ae0b0f6c4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shelter_data.drop_duplicates(subset=['geometry', 'year'])\n",
    "df_area = df[['geometry']].to_crs(epsg=3857)\n",
    "df_area['area_m2'] = df_area['geometry'].area\n",
    "df_area = df_area.to_crs(df.crs)\n",
    "shelter_data['area_m2'] = df_area['area_m2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30155025-2907-4ff8-9ea0-82463d23336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.sjoin(shelter_data, nuts, how='left', predicate='within')\n",
    "df = df.drop_duplicates(subset=['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc97be-669f-4f34-9167-04a94c3dcab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "officially_protected_pop_vln_pct = 0.2\n",
    "official_vln_pop_2024 = 851_346\n",
    "area_per_person = df[(df['is_shelter'] == 1) & (df['CNTR_CODE'] == 'LT') & (df['year'] == 2024)]['area_m2'].sum() / (officially_protected_pop_vln_pct * official_vln_pop_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5999ce6-0577-470c-a4a3-f5e051cf6184",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_shelters = df[(df['is_shelter'] != 1) & (df['could_be_shelter'] == 1)].groupby(['CNTR_CODE', 'NUTS_NAME', 'NUTS_ID', 'year']).agg({\n",
    "    'area_m2': ['sum'],\n",
    "    'could_be_shelter': ['sum']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db1c51-54b0-4ef0-80a0-90d2df5a505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing_shelters = df[(df['is_shelter'] == 1)].groupby(['CNTR_CODE', 'NUTS_NAME', 'NUTS_ID', 'year']).agg({\n",
    "    'area_m2': ['sum'],\n",
    "    'is_shelter': ['sum']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40475cdd-36d6-440d-b083-86bc4023d21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_new_shelters.join(df_existing_shelters, how='left', rsuffix='_existing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963b9f6-a0e7-4fc0-8e6d-578c18129793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['total_area'] = (df_total['area_m2'] + df_total['area_m2_existing'])['sum']\n",
    "df_total['total_count'] = (df_total['could_be_shelter'] + df_total['is_shelter'])['sum']\n",
    "df_total['pop_protected'] = (df_total['area_m2_existing'] / area_per_person)\n",
    "df_total['pop_could_protect'] = (df_total['total_area'] / area_per_person)\n",
    "df_total = df_total.droplevel(1, axis=1)\n",
    "df_pop = pd.read_csv('../data/nuts3_pop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a4665e-9a0c-46e9-b576-e27254e1c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.merge(df_total.reset_index(), df_pop, left_on='NUTS_ID', right_on='region_id', how='left')\n",
    "df_total['pop_pct_protected'] = df_total['pop_protected'] / df_total['total_pop']\n",
    "df_total['pop_pct_could_protect'] = df_total['pop_could_protect'] / df_total['total_pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee31b1-75b8-43f2-91bd-0e9a3c135635",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data = {}\n",
    "\n",
    "for region_id, group in df_total.groupby('NUTS_ID'):\n",
    "    group = group.sort_values('year')\n",
    "    years = list(group['year'])\n",
    "\n",
    "    # Construct region entry\n",
    "    region_data[region_id] = {\n",
    "        'name': group['NUTS_NAME'].iloc[0],\n",
    "        'country': group['country'].iloc[0],\n",
    "        'official_shelters': list(group['is_shelter']),\n",
    "        'suggested_shelters': list(group['could_be_shelter']),\n",
    "        'population_protected': list(group['pop_pct_protected']),\n",
    "        'years': years\n",
    "    }\n",
    "with open('../data/results.json', 'w') as json_file:\n",
    "    json.dump(region_data, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
